#!/usr/bin/python

import simplejson as json
import hashlib
from xml.etree import ElementTree as ETree
import psycopg2
from osgeo import ogr, osr
import argparse

import ipdb

#from .library import *

DATA_TYPE_MAPPING = {
    ogr.OFTInteger: {
        'interpretation': 'int64_t',
        'size': 8
    },
    ogr.OFTReal: {
        'interpretation': 'double',
        'size': 8
    },
    ogr.OFTDate: {
        'interpretation': 'int64_t',
        'size': 8
    },
    ogr.OFTTime: {
        'interpretation': 'int64_t',
        'size': 8
    },
    ogr.OFTDateTime: {
        'interpretation': 'int64_t',
        'size': 8
    }
}

def _init_argparser():

    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument(
        '-g', '--group-by',
        action='append',
        dest='group_by',
        help='attribute names to group by. Can be specified multiple times. If not specified, automatic grouping is done'
    )
    arg_parser.add_argument(
        '-l', '--layer',
        action='append',
        dest='layer',
        help='layer names to convert. Can be specified multiple times. If not specified, all layers of input file are processed'
    )

    arg_parser.add_argument(
        '-tz', '--timezone',
        dest='timezone',
        help='timezone for time and datetime values with no timezone. If not specified, local timezone is assumed'
    )

    arg_parser.add_argument(
        '-s', '--srid',
        dest='srid',
        default='0',
        help='SRID of the spatial coordinates'
    )

    arg_parser.add_argument(
        '-d', '--dsn',
        dest='dsn',
        required=True,
        help='database connection string'
    )
    arg_parser.add_argument(
        '-f', '--file',
        dest='input_file',
        required=True,
        help="OGR compatible file to be imported to pgPointCloud"
    )

    return arg_parser

def open_input_file(f):

    ogr.RegisterAll()

    ds = ogr.OpenShared(f, update=False)

    if ds is None:
        raise

    return ds

def open_db_connection(dsn):
    return psycopg2.connect(dsn)

def interpret_fields(layer, args):

    def add_coordinate(dimensions, coord):
        fldType = ogr.OFTReal
        dimensions.append({
            'index': None,
            'name': coord,
            'type': {
                'source': fldType,
                'dest': DATA_TYPE_MAPPING[fldType]
            }
        })

    fields = {
        'group_by': [],
        'ignore': [],
        'dimension': []
    }

    if layer.GetFeatureCount() < 1:
        raise

    add_coordinate(fields['dimension'], 'X')
    add_coordinate(fields['dimension'], 'Y')
    add_coordinate(fields['dimension'], 'Z')

    # use the first feature
    feat = layer.GetFeature(0)
    numFields = feat.GetFieldCount()

    for idx in xrange(numFields):
        fldDef = feat.GetFieldDefnRef(idx)

        fldInfo = {
            'index': idx,
            'name': fldDef.GetName(), 
        }

        fldType = fldDef.GetType()

        if args.group_by and fldInfo['name'] in args.group_by:
            fields['group_by'].append(fldInfo)
        elif fldType == ogr.OFTString:
            if not args.group_by:
                fields['group_by'].append(fldInfo)
            else:
                fields['ignore'].append(idx)
        elif fldType in DATA_TYPE_MAPPING:
            fldInfo['type'] = {
                'source': fldType,
                'dest': DATA_TYPE_MAPPING[fldType],
            }
            fields['dimension'].append(fldInfo)
        else:
            fields['ignore'].append(fldInfo)

    for key, indices in fields.iteritems():

        if len(indices) < 1:
            continue

        if key == 'ignore':
            label = 'ignored'
        elif key == 'group_by':
            label = 'grouped'
        else:
            continue

        print "The following fields will be %s:" % label

        for fld in indices:
            print "    %s" % fld['name']

    return fields

def build_pc_dimension(doc, dimension, index):

    pc_dimension = ETree.Element('pc:dimension')
    doc.append(pc_dimension)

    pc_position = ETree.Element('pc:position')
    pc_dimension.append(pc_position)
    pc_position.text = str(index)

    pc_name = ETree.Element('pc:name')
    pc_dimension.append(pc_name)
    pc_name.text = dimension['name']

    pc_size = ETree.Element('pc:size')
    pc_dimension.append(pc_size)
    pc_size.text = str(dimension['type']['dest']['size'])

    pc_interpretation = ETree.Element('pc:interpretation')
    pc_dimension.append(pc_interpretation)
    pc_interpretation.text = dimension['type']['dest']['interpretation']

    if dimension['type']['source'] in [
        ogr.OFTDate,
        ogr.OFTTime,
        ogr.OFTDateTime
    ]:
        pc_description = ETree.Element('pc:description')
        pc_dimension.append(pc_description)

        if dimension['type']['source'] == ogr.OFTDate:
            pc_description.text = 'date as number of seconds UTC from UNIX epoch to 00:00:00 of the date'
        elif dimension['type']['source'] == ogr.OFTTime:
            pc_description.text = 'time as number of seconds UTC from 00:00:00'
        elif dimension['type']['source'] == ogr.OFTDateTime:
            pc_description.text = 'datetime as number of seconds UTC from UNIX epoch'

    pc_metadata = ETree.Element('pc:metadata')
    pc_dimension.append(pc_metadata)

    # additional tags indicating that dimension is special 
    # date, time, datetime
    ogr_ = ETree.Element('ogr')
    pc_metadata.append(ogr_)

    data_type = ETree.Element('datatype')
    ogr_.append(data_type)
    data_type.text = ogr.GetFieldTypeName(dimension['type']['source'])

def build_pc_format(fields):
    XML_DECLARATION = '<?xml version="1.0" encoding="UTF-8"?>'

    pc_schema = ETree.Element('pc:PointCloudSchema')
    pc_schema.set('xmlns:pc', "http://pointcloud.org/schemas/PC/1.1")
    pc_schema.set('xmlns:xsi', "http://www.w3.org/2001/XMLSchema-instance")

    pc_metadata = ETree.Element('pc:metadata')
    pc_schema.append(pc_metadata)

    # compression
    Metadata = ETree.Element('Metadata')
    pc_metadata.append(Metadata)
    Metadata.set('name', 'compression')
    Metadata.text = 'dimensional'

    num_dimensions = 1

    for dimension in fields['dimension']:
        build_pc_dimension(pc_schema, dimension, num_dimensions)
        num_dimensions += 1

    return XML_DECLARATION + ETree.tostring(pc_schema)

def _add_pc_format(dbConn, pc_format):

    cursor = dbConn.cursor()

    try:

        # see if there are any available PCIDs
        cursor.execute("""
SELECT count(*) FROM pointcloud_formats
        """)
        pcid_count = cursor.fetchone()[0]

        # no vacancy
        if pcid_count == 65535:
            raise

        # next best PCID
        cursor.execute("""
WITH foo AS (
SELECT
	pcid,
	lead(pcid, 1, NULL) OVER (ORDER BY pcid DESC),
	pcid - lead(pcid, 1, NULL) OVER (ORDER BY pcid DESC) AS dist
FROM pointcloud_formats
ORDER BY pcid DESC
)
SELECT
	foo.pcid - 1 AS next_pcid
FROM foo
WHERE (foo.dist > 1 OR foo.dist IS NULL)
	AND (SELECT count(*) FROM pointcloud_formats AS srs WHERE srs.pcid = foo.pcid - 1) < 1
ORDER BY foo.pcid DESC
LIMIT 1
        """)
        next_pcid = cursor.fetchone()
        if next_pcid is None:
            next_pcid = 65535
        else:
            next_pcid = next_pcid[0]

        cursor.execute(
            'INSERT INTO pointcloud_formats (pcid, schema) VALUES (%s, %s)', (
                next_pcid,
                pc_format
            )
        )

    except psycopg2.Error:
        return None

    return next_pcid

def add_pc_format(dbConn, pc_format):

    max_count = 5
    count = 0

    pcid = None
    while pcid is None:
        count += 1
        pcid = _add_pc_format(dbConn, pc_format)

        if count > max_count:
            raise

    return pcid

def guess_layer_spatial_ref(self, layer):

    extent = layer.GetExtent()

    # is decimal degree?
    # -180 <= X <= 180
    # -90 <= X <= 90
    if (
        extent[0] >= -180. and
        extent[1] <= 180. and
        extent[2] >= -90. and
        extent[3] <= 90.
    ):
        # assume WGS84
        srs = osr.SpatialReference()
        srs.ImportFromEPSG(4326)
        return srs

    return False

def get_layer_spatial_ref(self, layer):

    srs = layer.GetSpatialRef()

    # no spatial reference system, attempt to guess
    if not srs:
        srs = self.guess_layer_spatial_ref(layer)
        # cannot guess, fail
        if not srs:
            return False

    # invalid spatial reference system
    elif srs.Validate() != 0:
        return False

    return True

def extract_group(feat, fields):

    num_group_by = len(fields['group_by'])
    group_list = []
    group_dict = {}
    for idx in xrange(num_group_by):
        group_list.append(feat.GetField(fields['group_by'][idx]['index']))
        group_dict[fields['group_by'][idx]['name']] = group_list[-1]

    return {
        hashlib.md5(json.dumps(group_list)).hexdigest(): group_dict
    }

def import_layer(dbConn, pcid, layer, fields, args):

    num_features = layer.GetFeatureCount()

    groups = []
    for idx in xrange(num_features):

        feat = layer.GetFeature(idx)

        # group first
        extract_group(feat, fields)

        # dimensions

def convert_layer(layer, dbConn, args):

    # process fields
    # find what to group by, what to ignore, what to process
    fields = interpret_fields(layer, args)

    if not fields['dimension']:
        return

    # build temporary layer format
    pc_format = build_pc_format(fields)

    # add format to database
    pcid = add_pc_format(dbConn, pc_format)

    import_layer(dbConn, pcid, layer, fields, args)

def convert_file(dsIn, dbConn, args):

    num_layers = dsIn.GetLayerCount()
    if num_layers < 1:
        return

    if args.layer:
        layers = args.layer
    else:
        layers = range(num_layers)

    for e in layers:

        if args.layer:
            layer = dsIn.GetLayerByName(e)
        else:
            layer = dsIn.GetLayerByIndex(e)

        if not layer:
            raise

        convert_layer(layer, dbConn, args)

def run(args):

    dsIn = open_input_file(args.input_file)
    dbConn = open_db_connection(args.dsn)

    try:
        convert_file(dsIn, dbConn, args)
        dbConn.commit()
    except:
        raise
    finally:
        dbConn.close()

if __name__ == '__main__':
    arg_parser = _init_argparser()
    args = arg_parser.parse_args()
    run(args)
