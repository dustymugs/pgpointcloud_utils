#!/usr/bin/python

import datetime
import pytz

import os

import simplejson as json
import hashlib
from xml.etree import ElementTree as ETree
import psycopg2
from psycopg2.extensions import AsIs
from osgeo import ogr, osr
import argparse

import random

import ipdb

#from .library import *

class OGR_TZ(datetime.tzinfo):

    def __init__(self, ogr_tz):

        self._hours = None
        self._minutes = None

        if ogr_tz <= 1:
            return

        offset = int(ogr_tz - 100 * 15)
        hours = int(offset / 60)
        minutes = int(abs(offset - hours * 60))

        if offset < 0:
            self._hours = -1 * abs(hours)
            self._minutes = -1 * minutes
        else:
            self._hours = hours
            self._minutes = minutes

    def utcoffset(self, dt):

        if self._hours is None:
            return None

        return datetime.timedelta(hours=self._hours, minutes=self._minutes)

    def tzname(self, dt):

        if self._hours is None:
            return None

        sign = '-' if self._hours < 0 else '+'

        return "%s%02f:%02f" % (
            sign,
            abs(self._hours),
            abs(self._minutes)
        )

    def dst(self, dt):
        return timedelta(0)

DATA_TYPE_MAPPING = {
    ogr.OFTInteger: {
        'interpretation': 'int64_t',
        'size': 8
    },
    ogr.OFTReal: {
        'interpretation': 'double',
        'size': 8
    },
    ogr.OFTDate: {
        'interpretation': 'int64_t',
        'size': 8
    },
    ogr.OFTTime: {
        'interpretation': 'int64_t',
        'size': 8
    },
    ogr.OFTDateTime: {
        'interpretation': 'int64_t',
        'size': 8
    }
}

Args = None
DSIn = None
DBConn = None

def _init_argparser():

    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument(
        '-g', '--group-by',
        action='append',
        dest='group_by',
        help="""Names of attributes to group by. Can be specified multiple 
        times. If not specified, automatic grouping is done"""
    )
    arg_parser.add_argument(
        '-l', '--layer',
        action='append',
        dest='layer',
        help="""Layer names to convert. Can be specified multiple times. If not
        specified, all layers of input file are processed"""
    )

    arg_parser.add_argument(
        '--date',
        action='append',
        dest='date',
        help="""Names of attributes to treat as Date values. Can be specified
        multiple times"""
    )
    arg_parser.add_argument(
        '--date-format',
        dest='date_format',
        help="""Date format. Formatting is based upon the Python datetime module's strptime()"""
    )
    arg_parser.add_argument(
        '--time',
        dest='time',
        help="""Names of attributes to treat as Time values. Can be specified
        multiple times"""
    )
    arg_parser.add_argument(
        '--time-format',
        dest='time_format',
        help="""Time format. Formatting is based upon the Python datetime module's strptime()"""
    )
    arg_parser.add_argument(
        '--datetime',
        dest='datetime',
        help="""Names of attributes to treat as DateTime values. Can be
        specified multiple times"""
    )
    arg_parser.add_argument(
        '--datetime-format',
        dest='datetime_format',
        help="""Datetime format. Formatting is based upon the Python datetime module's strptime()"""
    )

    # TODO
    #arg_parser.add_argument(
    #    '-tz', '--timezone',
    #    dest='timezone',
    #    help="""Timezone for time and datetime values with no timezone. If not
    #    specified, local timezone is assumed"""
    #)

    arg_parser.add_argument(
        '-p', '--pcid',
        dest='pcid',
        help='PCID of the pgPointCloud schema. This overrides the internal PCID schema creation'
    )

    arg_parser.add_argument(
        '-s', '--srid',
        dest='srid',
        help='SRID of the spatial coordinates X, Y, Z. This overrides the internal SRID estimation'
    )

    arg_parser.add_argument(
        '-t', '--tablename',
        dest='table_name',
        help="""Name of table to insert PcPatches into. If not specified, name
        of input file is used"""
    )

    arg_parser.add_argument(
        '-a', '--action',
        dest='table_action',
        default = 'create',
        help="""Action to take for the table. Possible actions are: (d)rop,
        (c)reate, (a)ppend. If not specified, (c)reate is the default"""
    )

    arg_parser.add_argument(
        '-d', '--dsn',
        dest='dsn',
        required=True,
        help='Database connection string'
    )

    arg_parser.add_argument(
        '-f', '--file',
        dest='input_file',
        required=True,
        help="OGR compatible file to be imported to pgPointCloud"
    )

    return arg_parser

def open_input_file(f):

    ogr.RegisterAll()

    ds = ogr.OpenShared(f, update=False)

    if ds is None:
        raise

    return ds

def open_db_connection(dsn):
    return psycopg2.connect(dsn)

def interpret_fields(layer):

    def add_coordinate(dimensions, coord):
        fldType = ogr.OFTReal
        dimensions.append({
            'index': None,
            'name': coord,
            'type': {
                'source': fldType,
                'dest': DATA_TYPE_MAPPING[fldType]
            }
        })

    fields = {
        'group_by': [],
        'ignore': [],
        'dimension': []
    }

    if layer.GetFeatureCount() < 1:
        raise

    add_coordinate(fields['dimension'], 'X')
    add_coordinate(fields['dimension'], 'Y')
    add_coordinate(fields['dimension'], 'Z')

    # use the first feature
    feat = layer.GetFeature(0)
    numFields = feat.GetFieldCount()

    for idx in xrange(numFields):
        fldDef = feat.GetFieldDefnRef(idx)

        fldInfo = {
            'index': idx,
            'name': fldDef.GetName(), 
        }

        fldType = fldDef.GetType()

        if Args.group_by and fldInfo['name'] in Args.group_by:
            fields['group_by'].append(fldInfo)
        elif fldType == ogr.OFTString:
            if not Args.group_by:
                fields['group_by'].append(fldInfo)
            else:
                fields['ignore'].append(idx)
        elif fldType in DATA_TYPE_MAPPING:
            fldInfo['type'] = {
                'source': fldType,
                'dest': DATA_TYPE_MAPPING[fldType],
            }
            fields['dimension'].append(fldInfo)
        else:
            fields['ignore'].append(fldInfo)

    for key, indices in fields.iteritems():

        if len(indices) < 1:
            continue

        if key == 'ignore':
            label = 'ignored'
        elif key == 'group_by':
            label = 'grouped'
        else:
            continue

        print "The following fields will be %s:" % label

        for fld in indices:
            print "    %s" % fld['name']

    return fields

def build_pc_dimension(doc, dimension, index):

    pc_dimension = ETree.Element('pc:dimension')
    doc.append(pc_dimension)

    pc_position = ETree.Element('pc:position')
    pc_dimension.append(pc_position)
    pc_position.text = str(index)

    pc_name = ETree.Element('pc:name')
    pc_dimension.append(pc_name)
    pc_name.text = dimension['name']

    pc_size = ETree.Element('pc:size')
    pc_dimension.append(pc_size)
    pc_size.text = str(dimension['type']['dest']['size'])

    pc_interpretation = ETree.Element('pc:interpretation')
    pc_dimension.append(pc_interpretation)
    pc_interpretation.text = dimension['type']['dest']['interpretation']

    if dimension['type']['source'] in [
        ogr.OFTDate,
        ogr.OFTTime,
        ogr.OFTDateTime
    ]:
        pc_description = ETree.Element('pc:description')
        pc_dimension.append(pc_description)

        if dimension['type']['source'] == ogr.OFTDate:
            pc_description.text = 'date as number of seconds UTC from UNIX epoch to 00:00:00 of the date'
        elif dimension['type']['source'] == ogr.OFTTime:
            pc_description.text = 'time as number of seconds UTC from 00:00:00'
        elif dimension['type']['source'] == ogr.OFTDateTime:
            pc_description.text = 'datetime as number of seconds UTC from UNIX epoch'

    pc_metadata = ETree.Element('pc:metadata')
    pc_dimension.append(pc_metadata)

    # additional tags indicating that dimension is special 
    # date, time, datetime
    ogr_ = ETree.Element('ogr')
    pc_metadata.append(ogr_)

    data_type = ETree.Element('datatype')
    ogr_.append(data_type)
    data_type.text = ogr.GetFieldTypeName(dimension['type']['source'])

def build_pc_schema(fields):
    XML_DECLARATION = '<?xml version="1.0" encoding="UTF-8"?>'

    pc_schema = ETree.Element('pc:PointCloudSchema')
    pc_schema.set('xmlns:pc', "http://pointcloud.org/schemas/PC/1.1")
    pc_schema.set('xmlns:xsi', "http://www.w3.org/2001/XMLSchema-instance")

    pc_metadata = ETree.Element('pc:metadata')
    pc_schema.append(pc_metadata)

    # compression
    Metadata = ETree.Element('Metadata')
    pc_metadata.append(Metadata)
    Metadata.set('name', 'compression')
    Metadata.text = 'dimensional'

    num_dimensions = 1

    for dimension in fields['dimension']:
        build_pc_dimension(pc_schema, dimension, num_dimensions)
        num_dimensions += 1

    return XML_DECLARATION + ETree.tostring(pc_schema)

def _add_pc_schema(pc_schema, srid):

    cursor = DBConn.cursor()

    try:

        # check if this schema already exists
        cursor.execute("""
SELECT
    pcid
FROM pointcloud_formats
WHERE schema = %s
        """, [pc_schema])
        # it does exist, use
        if cursor.rowcount > 0:
            return cursor.fetchone()[0]

        # see if there are any available PCIDs
        cursor.execute("""
SELECT count(*) FROM pointcloud_formats
        """)
        pcid_count = cursor.fetchone()[0]

        # no vacancy
        if pcid_count == 65535:
            raise

        # next best PCID
        cursor.execute("""
WITH foo AS (
SELECT
	pcid,
	lead(pcid, 1, NULL) OVER (ORDER BY pcid DESC),
	pcid - lead(pcid, 1, NULL) OVER (ORDER BY pcid DESC) AS dist
FROM pointcloud_formats
ORDER BY pcid DESC
)
SELECT
	foo.pcid - 1 AS next_pcid
FROM foo
WHERE (foo.dist > 1 OR foo.dist IS NULL)
	AND (SELECT count(*) FROM pointcloud_formats AS srs WHERE srs.pcid = foo.pcid - 1) < 1
ORDER BY foo.pcid DESC
LIMIT 1
        """)
        if cursor.rowcount > 0:
            next_pcid = cursor.fetchone()[0]
        else:
            next_pcid = 65535

        cursor.execute(
            'INSERT INTO pointcloud_formats (pcid, srid, schema) VALUES (%s, %s, %s)', (
                next_pcid,
                srid,
                pc_schema
            )
        )

    except psycopg2.Error:
        DBConn.rollback()
        return None
    finally:
        cursor.close()

    return next_pcid

def add_pc_schema(pc_schema, srid=0):

    max_count = 5
    count = 0

    pcid = None
    while pcid is None:
        count += 1
        pcid = _add_pc_schema(pc_schema, srid)

        if count > max_count:
            raise

    return pcid

def guess_layer_spatial_ref(layer):

    extent = layer.GetExtent()

    # is decimal degree?
    # -180 <= X <= 180
    # -90 <= X <= 90
    if (
        extent[0] >= -180. and
        extent[1] <= 180. and
        extent[2] >= -90. and
        extent[3] <= 90.
    ):
        # assume WGS84
        return 4326

    # cannot guess, return zero
    return 0

def _get_postgis_srid(proj4):

    cursor = DBConn.cursor()

    try:
        cursor.execute("""
SELECT
    srid
FROM spatial_ref_sys
WHERE proj4text = %s
        """, [proj4])

        if cursor.rowcount > 0:
            srid = cursor.fetchone()[0]
        else:
            srid = 0

    except psycopg2.Error:
        DBConn.rollback()
        return 0
    finally:
        cursor.close()

    return srid

def get_layer_srid(layer):

    if Args.srid is not None:
        return Args.srid

    srs = layer.GetSpatialRef()

    # no spatial reference system, attempt to guess
    if not srs:
        return guess_layer_spatial_ref(layer)

    # invalid spatial reference system
    elif srs.Validate() != 0:
        return 0

    # try to get the PostGIS SRID
    srid = _get_postgis_srid(srs.ExportToProj4())

    return srid

def extract_group(feat, fields):

    num_group_by = len(fields['group_by'])
    group_list = []
    group_dict = {}
    for idx in xrange(num_group_by):
        group_list.append(feat.GetField(fields['group_by'][idx]['index']))
        group_dict[fields['group_by'][idx]['name']] = group_list[-1]

    #return hashlib.md5(json.dumps(group_list)).hexdigest(), group_dict
    return group_dict

def create_temp_table():

    table_name = (
        'temp_' +
        ''.join(random.choice('0123456789abcdefghijklmnopqrstuvwxyz') for i in range(16))
    )

    cursor = DBConn.cursor()

    try:

        cursor.execute("""
CREATE TEMPORARY TABLE "%s" (
    id BIGSERIAL PRIMARY KEY,
    pt PCPOINT,
    group_by TEXT
)
ON COMMIT DROP;
        """, [AsIs(table_name)])

    except psycopg2.Error:
        DBConn.rollback()
        raise
    finally:
        cursor.close()

    return table_name

def build_pcpoint_from_feature(feat, fields):

    geom = feat.geometry()
    if geom.GetGeometryType() != ogr.wkbPoint:
        geom = geom.Centroid()

    vals = []
    for dimension in fields['dimension']:

        if dimension['name'] in ['X', 'Y', 'Z']:
            func = getattr(geom, 'Get' + dimension['name'])
            vals.append(func())
        elif dimension['type']['source'] in [
            ogr.OFTDate,
            ogr.OFTTime,
            ogr.OFTDateTime
        ]:
            val = feat.GetFieldAsDateTime(dimension['index'])

            # date is converted to number of seconds UTC from UNIX epoch
            if dimension['type']['source'] == ogr.OFTDate:
                vals.append((
                    datetime.datetime(*val[0:3]) -
                    datetime.datetime(1970, 1, 1)
                ).total_seconds())
            # time is converted to number of seconds from 00:00:00 UTC
            elif dimension['type']['source'] == ogr.OFTTime:
                tz = OGR_TZ(val[-1])
                vals.append((
                    datetime.time(*val[3:6], tzinfo=tz).astimezone(pytz.UTC) -
                    datetime.time(0, 0, 0, tzinfo=pytz.UTC)
                ).total_seconds())
            elif dimension['type']['source'] == ogr.OFTDateTime:
                tz = OGR_TZ(val[-1])
                vals.append((
                    datetime.datetime(*val[0:6], tzinfo=tz).astimezone(pytz.UTC) -
                    datetime.datetime(1970, 1, 1)
                ).total_seconds())
        else:
            vals.append(feat.GetField(dimension['index']))
        
    return vals

def insert_pcpoint(table_name, pcid, group, vals):

    cursor = DBConn.cursor()

    try:

        group_str = json.dumps(group)

        cursor.execute("""
INSERT INTO "%s" (pt, group_by)
VALUES (PC_MakePoint(%s, %s), %s)
        """, [
            AsIs(table_name),
            pcid,
            vals,
            group_str
        ])

    except psycopg2.Error:
        DBConn.rollback()
        raise
    finally:
        cursor.close()

def insert_pcpatches(file_table, temp_table, layer):

    layer_name = layer.GetName()

    cursor = DBConn.cursor()

    try:

        cursor.execute("""
INSERT INTO "%s" (layer_name, group_by, pa) 
SELECT
    layer_name,
    group_by::json,
    pa
FROM (
    SELECT
        %s AS layer_name,
        group_by,
        PC_Patch(pt) AS pa
    FROM "%s"
    GROUP BY 1, 2
) sub
        """, [
            AsIs(file_table),
            layer_name,
            AsIs(temp_table),
        ])

    except psycopg2.Error:
        DBConn.rollback()
        raise
    finally:
        cursor.close()

def import_layer(layer, file_table, pcid, fields):

    num_features = layer.GetFeatureCount()

    # create temporary table for layer
    temp_table = create_temp_table()

    # iterate over features
    for idx in xrange(num_features):

        feat = layer.GetFeature(idx)

        # get group
        group = extract_group(feat, fields)

        # build pcpoint values
        vals = build_pcpoint_from_feature(feat, fields)

        # insert
        insert_pcpoint(temp_table, pcid, group, vals)

    # build patches for layer by distinct group
    insert_pcpatches(file_table, temp_table, layer)

    return True

def convert_layer(layer, file_table):

    # process fields
    # find what to group by, what to ignore, what to process
    fields = interpret_fields(layer)

    if not fields['dimension']:
        return

    # get SRID of layer
    srid = get_layer_srid(layer)

    # specified pcid
    if Args.pcid is not None:
        pcid = Args.pcid
    else:
        # build pointcloud schema
        pc_schema = build_pc_schema(fields)

        # add schema to database
        pcid = add_pc_schema(pc_schema, srid)

    # do the actual import
    import_layer(layer, file_table, pcid, fields)

def create_file_table():

    if Args.table_name is not None:
        table_name = Args.table_name
    else:
        table_name = os.path.splitext(os.path.basename(DSIn.name))[0]

    cursor = DBConn.cursor()

    action = Args.table_action[0]

    try:

        # append to existing table, check that table exists
        if action == 'a':
            try:
                cursor.execute("""
SELECT 1 FROM "%s"
                """, [AsIs(table_name)])
            except psycopg2.Error:
                raise Exception('Table not found: %s' % table_name)

            return table_name

        # drop table
        if action == 'd':
            cursor.execute("""
DROP TABLE IF EXISTS "%s"
            """, [AsIs(table_name)])

        cursor.execute("""
CREATE TABLE "%s" (
    id BIGSERIAL PRIMARY KEY,
    pa PCPATCH,
    layer_name TEXT,
    group_by JSON
)
        """, [AsIs(table_name)])

    except psycopg2.Error:
        DBConn.rollback()
        raise
    finally:
        cursor.close()

    return table_name

def convert_file():

    file_table = create_file_table()

    num_layers = DSIn.GetLayerCount()
    if num_layers < 1:
        return

    if Args.layer:
        layers = Args.layer
    else:
        layers = range(num_layers)

    for e in layers:

        if Args.layer:
            layer = DSIn.GetLayerByName(e)
        else:
            layer = DSIn.GetLayerByIndex(e)

        if not layer:
            raise

        convert_layer(layer, file_table)

def run():

    global DSIn
    global DBConn

    DSIn = open_input_file(Args.input_file)
    DBConn = open_db_connection(Args.dsn)

    try:
        convert_file()
        DBConn.commit()
    except:
        DBConn.rollback()
        raise
    finally:
        DBConn.close()

if __name__ == '__main__':
    arg_parser = _init_argparser()
    Args = arg_parser.parse_args()
    run()
